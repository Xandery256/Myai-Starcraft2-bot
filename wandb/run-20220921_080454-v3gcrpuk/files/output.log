On iteration:  0
RESETTING ENVIRONMENT
Logging to logs/test_run/PPO_0
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.07e+03 |
|    ep_rew_mean     | -315     |
| time/              |          |
|    fps             | 12       |
|    iterations      | 1        |
|    time_elapsed    | 160      |
|    total_timesteps | 43008    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.07e+03    |
|    ep_rew_mean          | -315        |
| time/                   |             |
|    fps                  | 8           |
|    iterations           | 2           |
|    time_elapsed         | 471         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.104089245 |
|    clip_fraction        | 0.539       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.503      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0924     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0643     |
|    value_loss           | 0.0947      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.07e+03   |
|    ep_rew_mean          | -315       |
| time/                   |            |
|    fps                  | 8          |
|    iterations           | 3          |
|    time_elapsed         | 738        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.11589119 |
|    clip_fraction        | 0.485      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | 0.0684     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.111     |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0631    |
|    value_loss           | 0.661      |
----------------------------------------
RESETTING ENVIRONMENT
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.65e+03   |
|    ep_rew_mean          | -263       |
| time/                   |            |
|    fps                  | 8          |
|    iterations           | 4          |
|    time_elapsed         | 967        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.09365638 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.31      |
|    explained_variance   | 0.224      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0788    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0718    |
|    value_loss           | 0.939      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.65e+03    |
|    ep_rew_mean          | -263        |
| time/                   |             |
|    fps                  | 8           |
|    iterations           | 5           |
|    time_elapsed         | 1197        |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.078817256 |
|    clip_fraction        | 0.424       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.0517      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0405     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 777         |
-----------------------------------------
On iteration:  1
Logging to logs/test_run/PPO_0
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.65e+03 |
|    ep_rew_mean     | -263     |
| time/              |          |
|    fps             | 8        |
|    iterations      | 1        |
|    time_elapsed    | 237      |
|    total_timesteps | 53248    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.65e+03    |
|    ep_rew_mean          | -263        |
| time/                   |             |
|    fps                  | 7           |
|    iterations           | 2           |
|    time_elapsed         | 540         |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.056131918 |
|    clip_fraction        | 0.386       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.265       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0752      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0607     |
|    value_loss           | 1.2         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.65e+03    |
|    ep_rew_mean          | -263        |
| time/                   |             |
|    fps                  | 6           |
|    iterations           | 3           |
|    time_elapsed         | 884         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.033014696 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.212       |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0438     |
|    value_loss           | 1.88        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.65e+03    |
|    ep_rew_mean          | -263        |
| time/                   |             |
|    fps                  | 7           |
|    iterations           | 4           |
|    time_elapsed         | 1169        |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.008665382 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.244       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.637       |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 8.47        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.65e+03    |
|    ep_rew_mean          | -263        |
| time/                   |             |
|    fps                  | 7           |
|    iterations           | 5           |
|    time_elapsed         | 1364        |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.020970212 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.447       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.346       |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 2.71        |
-----------------------------------------
On iteration:  2
Logging to logs/test_run/PPO_0
RESETTING ENVIRONMENT
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.34e+03 |
|    ep_rew_mean     | -35.3    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 1        |
|    time_elapsed    | 126      |
|    total_timesteps | 63488    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 8.34e+03   |
|    ep_rew_mean          | -35.3      |
| time/                   |            |
|    fps                  | 10         |
|    iterations           | 2          |
|    time_elapsed         | 393        |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.06493452 |
|    clip_fraction        | 0.428      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.29      |
|    explained_variance   | 0.0533     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.459      |
|    n_updates            | 310        |
|    policy_gradient_loss | -5.54e-05  |
|    value_loss           | 723        |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 8.34e+03   |
|    ep_rew_mean          | -35.3      |
| time/                   |            |
|    fps                  | 9          |
|    iterations           | 3          |
|    time_elapsed         | 629        |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.12206985 |
|    clip_fraction        | 0.54       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.0471     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0858    |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.0617    |
|    value_loss           | 0.251      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 8.34e+03  |
|    ep_rew_mean          | -35.3     |
| time/                   |           |
|    fps                  | 9         |
|    iterations           | 4         |
|    time_elapsed         | 822       |
|    total_timesteps      | 69632     |
| train/                  |           |
|    approx_kl            | 0.1973228 |
|    clip_fraction        | 0.512     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.16     |
|    explained_variance   | 0.207     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.119    |
|    n_updates            | 330       |
|    policy_gradient_loss | -0.0793   |
|    value_loss           | 0.641     |
---------------------------------------
RESETTING ENVIRONMENT
Traceback (most recent call last):
  File "load-train-model.py", line 44, in <module>
    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f"PPO")
  File "C:\Users\ary93\AppData\Local\Programs\Python\Python38\lib\site-packages\stable_baselines3\ppo\ppo.py", line 310, in learn
    return super().learn(
  File "C:\Users\ary93\AppData\Local\Programs\Python\Python38\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 247, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\ary93\AppData\Local\Programs\Python\Python38\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 175, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "C:\Users\ary93\AppData\Local\Programs\Python\Python38\lib\site-packages\stable_baselines3\common\vec_env\base_vec_env.py", line 162, in step
    return self.step_wait()
  File "C:\Users\ary93\AppData\Local\Programs\Python\Python38\lib\site-packages\stable_baselines3\common\vec_env\vec_transpose.py", line 95, in step_wait
    observations, rewards, dones, infos = self.venv.step_wait()
  File "C:\Users\ary93\AppData\Local\Programs\Python\Python38\lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py", line 43, in step_wait
    obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(
  File "C:\Users\ary93\AppData\Local\Programs\Python\Python38\lib\site-packages\stable_baselines3\common\monitor.py", line 90, in step
    observation, reward, done, info = self.env.step(action)
  File "C:\Users\ary93\OneDrive - Bob Jones University\Year 4 S1\CpS 499\MyAi\sc2env.py", line 27, in step
    state_rwd_action = pickle.load(f)
KeyboardInterrupt