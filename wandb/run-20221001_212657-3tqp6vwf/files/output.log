Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Wrapping the env in a VecTransposeImage.
On iteration:  0
RESETTING ENVIRONMENT
Logging to logs/test_run3/PPO_0
-----------------------------
| time/              |      |
|    fps             | 18   |
|    iterations      | 1    |
|    time_elapsed    | 108  |
|    total_timesteps | 2048 |
-----------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 13          |
|    iterations           | 2           |
|    time_elapsed         | 299         |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.014881454 |
|    clip_fraction        | 0.0934      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.0387      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0729     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0305     |
|    value_loss           | 0.0185      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 12          |
|    iterations           | 3           |
|    time_elapsed         | 496         |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.038811147 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.27       |
|    explained_variance   | 0.0515      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0643     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0512     |
|    value_loss           | 0.16        |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 12         |
|    iterations           | 4          |
|    time_elapsed         | 673        |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.03759869 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.25      |
|    explained_variance   | 0.113      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0943    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0509    |
|    value_loss           | 0.283      |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 11          |
|    iterations           | 5           |
|    time_elapsed         | 854         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.032648847 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.21       |
|    explained_variance   | -0.0316     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0518     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0576     |
|    value_loss           | 1.38        |
-----------------------------------------
On iteration:  1
Logging to logs/test_run3/PPO_0
------------------------------
| time/              |       |
|    fps             | 14    |
|    iterations      | 1     |
|    time_elapsed    | 141   |
|    total_timesteps | 12288 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 12          |
|    iterations           | 2           |
|    time_elapsed         | 337         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.040606685 |
|    clip_fraction        | 0.315       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.188       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0617     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0677     |
|    value_loss           | 0.517       |
-----------------------------------------
RESETTING ENVIRONMENT
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.53e+04    |
|    ep_rew_mean          | 272         |
| time/                   |             |
|    fps                  | 12          |
|    iterations           | 3           |
|    time_elapsed         | 481         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.030173458 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.19       |
|    explained_variance   | 0.163       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.726       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0422     |
|    value_loss           | 3.55        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.53e+04    |
|    ep_rew_mean          | 272         |
| time/                   |             |
|    fps                  | 12          |
|    iterations           | 4           |
|    time_elapsed         | 634         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.037236635 |
|    clip_fraction        | 0.382       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.23       |
|    explained_variance   | 0.0131      |
|    learning_rate        | 0.0003      |
|    loss                 | 681         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00975    |
|    value_loss           | 877         |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.53e+04   |
|    ep_rew_mean          | 272        |
| time/                   |            |
|    fps                  | 12         |
|    iterations           | 5          |
|    time_elapsed         | 818        |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.08171572 |
|    clip_fraction        | 0.521      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.13      |
|    explained_variance   | -0.0875    |
|    learning_rate        | 0.0003     |
|    loss                 | -0.1       |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0745    |
|    value_loss           | 0.0673     |
----------------------------------------
On iteration:  2
Logging to logs/test_run3/PPO_0
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.53e+04 |
|    ep_rew_mean     | 272      |
| time/              |          |
|    fps             | 16       |
|    iterations      | 1        |
|    time_elapsed    | 122      |
|    total_timesteps | 22528    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.53e+04   |
|    ep_rew_mean          | 272        |
| time/                   |            |
|    fps                  | 11         |
|    iterations           | 2          |
|    time_elapsed         | 350        |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.09924783 |
|    clip_fraction        | 0.534      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.1       |
|    explained_variance   | -0.0181    |
|    learning_rate        | 0.0003     |
|    loss                 | -0.134     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0879    |
|    value_loss           | 0.357      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.53e+04    |
|    ep_rew_mean          | 272         |
| time/                   |             |
|    fps                  | 10          |
|    iterations           | 3           |
|    time_elapsed         | 565         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.072220184 |
|    clip_fraction        | 0.459       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | -0.22       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0687     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.063      |
|    value_loss           | 0.662       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.53e+04    |
|    ep_rew_mean          | 272         |
| time/                   |             |
|    fps                  | 11          |
|    iterations           | 4           |
|    time_elapsed         | 703         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.064267926 |
|    clip_fraction        | 0.456       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.09       |
|    explained_variance   | 0.114       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0578     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0818     |
|    value_loss           | 0.969       |
-----------------------------------------
RESETTING ENVIRONMENT
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.49e+04   |
|    ep_rew_mean          | 182        |
| time/                   |            |
|    fps                  | 12         |
|    iterations           | 5          |
|    time_elapsed         | 844        |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.03895489 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.11      |
|    explained_variance   | 0.598      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0294    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0711    |
|    value_loss           | 0.359      |
----------------------------------------
On iteration:  3
Logging to logs/test_run3/PPO_0
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.49e+04 |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 20       |
|    iterations      | 1        |
|    time_elapsed    | 100      |
|    total_timesteps | 32768    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.49e+04   |
|    ep_rew_mean          | 182        |
| time/                   |            |
|    fps                  | 15         |
|    iterations           | 2          |
|    time_elapsed         | 265        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.09785412 |
|    clip_fraction        | 0.566      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.78      |
|    explained_variance   | 0.0549     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.109     |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0768    |
|    value_loss           | 0.0894     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.49e+04   |
|    ep_rew_mean          | 182        |
| time/                   |            |
|    fps                  | 14         |
|    iterations           | 3          |
|    time_elapsed         | 417        |
|    total_timesteps      | 36864      |
| train/                  |            |
|    approx_kl            | 0.10270492 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.62      |
|    explained_variance   | 0.136      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.118     |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0763    |
|    value_loss           | 0.584      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.49e+04   |
|    ep_rew_mean          | 182        |
| time/                   |            |
|    fps                  | 13         |
|    iterations           | 4          |
|    time_elapsed         | 609        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.06922121 |
|    clip_fraction        | 0.453      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.94      |
|    explained_variance   | 0.472      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0734    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0645    |
|    value_loss           | 0.304      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.49e+04   |
|    ep_rew_mean          | 182        |
| time/                   |            |
|    fps                  | 12         |
|    iterations           | 5          |
|    time_elapsed         | 844        |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.07647469 |
|    clip_fraction        | 0.414      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.86      |
|    explained_variance   | 0.285      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0875    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0575    |
|    value_loss           | 0.368      |
----------------------------------------
On iteration:  4
Logging to logs/test_run3/PPO_0
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.49e+04 |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 13       |
|    iterations      | 1        |
|    time_elapsed    | 147      |
|    total_timesteps | 43008    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.49e+04   |
|    ep_rew_mean          | 182        |
| time/                   |            |
|    fps                  | 12         |
|    iterations           | 2          |
|    time_elapsed         | 341        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.09568995 |
|    clip_fraction        | 0.499      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.88      |
|    explained_variance   | 0.497      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.106     |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0851    |
|    value_loss           | 0.246      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.49e+04   |
|    ep_rew_mean          | 182        |
| time/                   |            |
|    fps                  | 13         |
|    iterations           | 3          |
|    time_elapsed         | 461        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.11473528 |
|    clip_fraction        | 0.476      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.81      |
|    explained_variance   | 0.352      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0903    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0778    |
|    value_loss           | 0.41       |
----------------------------------------
RESETTING ENVIRONMENT
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.6e+04    |
|    ep_rew_mean          | 220        |
| time/                   |            |
|    fps                  | 13         |
|    iterations           | 4          |
|    time_elapsed         | 595        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.10140972 |
|    clip_fraction        | 0.469      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.86      |
|    explained_variance   | 0.416      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0492    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0692    |
|    value_loss           | 0.29       |
----------------------------------------
RESETTING ENVIRONMENT
Traceback (most recent call last):
  File "C:\Users\yerke\OneDrive - Bob Jones University\Year 4 S1\CpS 499\MyAi\train.py", line 45, in <module>
    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f"PPO")
  File "C:\Users\yerke\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\ppo\ppo.py", line 304, in learn
    return super(PPO, self).learn(
  File "C:\Users\yerke\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 250, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\yerke\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 178, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "C:\Users\yerke\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\vec_env\base_vec_env.py", line 162, in step
    return self.step_wait()
  File "C:\Users\yerke\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\vec_env\vec_transpose.py", line 95, in step_wait
    observations, rewards, dones, infos = self.venv.step_wait()
  File "C:\Users\yerke\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py", line 43, in step_wait
    obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(
  File "C:\Users\yerke\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\monitor.py", line 90, in step
    observation, reward, done, info = self.env.step(action)
  File "C:\Users\yerke\OneDrive - Bob Jones University\Year 4 S1\CpS 499\MyAi\sc2env.py", line 48, in step
    with open('state_rwd_action.pkl', 'rb') as f:
KeyboardInterrupt