On iteration:  0
RESETTING ENVIRONMENT
Logging to logs/test_run/PPO_0
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.83e+03 |
|    ep_rew_mean     | -398     |
| time/              |          |
|    fps             | 12       |
|    iterations      | 1        |
|    time_elapsed    | 158      |
|    total_timesteps | 22528    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.83e+03   |
|    ep_rew_mean          | -398       |
| time/                   |            |
|    fps                  | 8          |
|    iterations           | 2          |
|    time_elapsed         | 479        |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.07962769 |
|    clip_fraction        | 0.494      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.62      |
|    explained_variance   | 0.515      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.11      |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0685    |
|    value_loss           | 0.0304     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.83e+03   |
|    ep_rew_mean          | -398       |
| time/                   |            |
|    fps                  | 8          |
|    iterations           | 3          |
|    time_elapsed         | 743        |
|    total_timesteps      | 26624      |
| train/                  |            |
|    approx_kl            | 0.06764661 |
|    clip_fraction        | 0.473      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.52      |
|    explained_variance   | 0.0364     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.105     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0629    |
|    value_loss           | 0.421      |
----------------------------------------
RESETTING ENVIRONMENT
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 6.07e+03 |
|    ep_rew_mean          | -315     |
| time/                   |          |
|    fps                  | 8        |
|    iterations           | 4        |
|    time_elapsed         | 965      |
|    total_timesteps      | 28672    |
| train/                  |          |
|    approx_kl            | 0.100059 |
|    clip_fraction        | 0.546    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.58    |
|    explained_variance   | -0.0493  |
|    learning_rate        | 0.0003   |
|    loss                 | -0.137   |
|    n_updates            | 130      |
|    policy_gradient_loss | -0.0928  |
|    value_loss           | 0.421    |
--------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.07e+03    |
|    ep_rew_mean          | -315        |
| time/                   |             |
|    fps                  | 8           |
|    iterations           | 5           |
|    time_elapsed         | 1274        |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.032616325 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0.0187      |
|    learning_rate        | 0.0003      |
|    loss                 | 239         |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 830         |
-----------------------------------------
On iteration:  1
Logging to logs/test_run/PPO_0
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.07e+03 |
|    ep_rew_mean     | -315     |
| time/              |          |
|    fps             | 10       |
|    iterations      | 1        |
|    time_elapsed    | 186      |
|    total_timesteps | 32768    |
---------------------------------
RESETTING ENVIRONMENT
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.06e+03   |
|    ep_rew_mean          | -314       |
| time/                   |            |
|    fps                  | 9          |
|    iterations           | 2          |
|    time_elapsed         | 437        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.08460425 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.53      |
|    explained_variance   | 0.878      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0837    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0648    |
|    value_loss           | 0.436      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.06e+03    |
|    ep_rew_mean          | -314        |
| time/                   |             |
|    fps                  | 7           |
|    iterations           | 3           |
|    time_elapsed         | 793         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.061264947 |
|    clip_fraction        | 0.502       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.21        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.77e+03    |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00373    |
|    value_loss           | 836         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.06e+03    |
|    ep_rew_mean          | -314        |
| time/                   |             |
|    fps                  | 7           |
|    iterations           | 4           |
|    time_elapsed         | 1147        |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.112133175 |
|    clip_fraction        | 0.623       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | -0.179      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0982     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0689     |
|    value_loss           | 0.594       |
-----------------------------------------
RESETTING ENVIRONMENT
Traceback (most recent call last):
  File "load-train-model.py", line 44, in <module>
    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f"PPO")
  File "C:\Users\ary93\AppData\Local\Programs\Python\Python38\lib\site-packages\stable_baselines3\ppo\ppo.py", line 310, in learn
    return super().learn(
  File "C:\Users\ary93\AppData\Local\Programs\Python\Python38\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 247, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\ary93\AppData\Local\Programs\Python\Python38\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 175, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "C:\Users\ary93\AppData\Local\Programs\Python\Python38\lib\site-packages\stable_baselines3\common\vec_env\base_vec_env.py", line 162, in step
    return self.step_wait()
  File "C:\Users\ary93\AppData\Local\Programs\Python\Python38\lib\site-packages\stable_baselines3\common\vec_env\vec_transpose.py", line 95, in step_wait
    observations, rewards, dones, infos = self.venv.step_wait()
  File "C:\Users\ary93\AppData\Local\Programs\Python\Python38\lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py", line 43, in step_wait
    obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(
  File "C:\Users\ary93\AppData\Local\Programs\Python\Python38\lib\site-packages\stable_baselines3\common\monitor.py", line 90, in step
    observation, reward, done, info = self.env.step(action)
  File "C:\Users\ary93\OneDrive - Bob Jones University\Year 4 S1\CpS 499\MyAi\sc2env.py", line 26, in step
    with open('state_rwd_action.pkl', 'rb') as f:
KeyboardInterrupt